{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reorganização concluída!\n"
     ]
    }
   ],
   "source": [
    "# REORGANIZANDO DADOS\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os # permite operações no sistema de arquivos e a manipulação de diretórios\n",
    "import shutil # peermite copiar e excluir arquivos\n",
    "\n",
    "\n",
    "\"\"\" \n",
    "    Dataset: https://www.kaggle.com/datasets/aksha05/flower-image-dataset/data\n",
    "    \n",
    "    ================================DETALHES==================================\n",
    "    Esse dataset não veio separando as classes em pastas, o que dificulta como  ia vai identificar cada classe,\n",
    "    então eu reorganizei em pastas com um código em python, o nome da classe estava no arquivo JPG antes do _ . \n",
    "        Por exemplo : daises_0010.jpg, daises_0011.jpg, gardenias_0002.jpg\n",
    "\"\"\"\n",
    "\n",
    "# Caminho para a pasta com as imagens\n",
    "dataset_path = 'flowers'\n",
    "# Caminho para onde as subpastas serão criadas (opcional, se quiser em outro local)\n",
    "output_path = 'flowers_reorganized'  # Modifique conforme necessário\n",
    "\n",
    "# Criar a pasta de saída se ela não existir\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "\n",
    "# Lista de arquivos na pasta, pega todos os arquivos de flowers\n",
    "image_files = os.listdir(dataset_path)\n",
    "\n",
    "# Criação das subpastas e movimentação dos arquivos\n",
    "for image_file in image_files:\n",
    "\n",
    "    \"\"\" \n",
    "        Se image_file for \"daisies_001.jpg\", a função split('_') vai gerar a lista ['daisies', '001.jpg'].\n",
    "        [0]: Esse índice acessa o primeiro elemento da lista gerada pela função split(). No exemplo acima, o primeiro elemento é 'daisies'.\n",
    "    \"\"\"\n",
    "    # Extrai a classe do nome do arquivo\n",
    "\n",
    "    class_name = image_file.split('_')[0]\n",
    "    \n",
    "    # Cria a pasta da classe se ela não existir\n",
    "    class_folder = os.path.join(output_path, class_name)\n",
    "    if not os.path.exists(class_folder):\n",
    "        os.makedirs(class_folder)\n",
    "    \n",
    "    # Move o arquivo para a pasta correspondente\n",
    "    src_path = os.path.join(dataset_path, image_file)\n",
    "    dst_path = os.path.join(class_folder, image_file)\n",
    "    \n",
    "    # Move o arquivo para a nova subpasta\n",
    "    shutil.move(src_path, dst_path)\n",
    "\n",
    "print(\"Reorganização concluída!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\n    ========================== OBJETIVOS DA IA ==============================\\n\\n    ACURACY : VALOR PRÓXIMO A 1\\n        Interpretação: A acurácia mede a proporção de previsões corretas do modelo em relação ao total de previsões feitas. O valor ideal seria 1 (ou 100%), o que significa que o modelo acertou todas as previsões.\\n        Em problemas de classificação, uma acurácia de 1 (ou 100%) significa que o modelo está fazendo previsões perfeitas.\\n    LOSS (PERDA) : VALOR PRÓXIMO A 0\\n        Interpretação: A função de perda (loss function) calcula a diferença entre as previsões do modelo e os valores reais (verdadeiros). Quanto menor for o valor da perda, melhor o modelo está se ajustando aos dados.\\n        Um valor de 0 indica que o modelo está fazendo previsões perfeitas, ou seja, sem erro. Na prática, o valor de perda não é geralmente 0, mas deve ser o menor possível.\\n\\n    ========================== DICAS ==============================\\n\\n    ALFA (TAXA DE APRENDIZADO) : \\n        VALOR ALTO : Comportamento: Se o valor de alfa for muito alto, o modelo pode fazer grandes atualizações nos pesos em cada iteração. Isso pode levar a uma convergência instável, onde o modelo \"salta\" para frente e para trás sem realmente encontrar o ponto ótimo.\\n\\n        VALOR BAIXO : Se o valor de alfa for muito baixo, o modelo fará atualizações muito pequenas nos pesos, o que pode levar a um processo de treinamento muito lento. Isso pode aumentar o tempo de treinamento e, em alguns casos, pode fazer com que o modelo demore mais para alcançar o ponto ótimo.\\n\\n        IDEAL: Ajuste dinâmico: Muitas vezes, usa-se um learning rate scheduler ou técnicas como learning rate annealing ou Adam, que ajustam automaticamente a taxa de aprendizado durante o treinamento, começando com uma taxa de aprendizado relativamente alta e diminuindo ao longo das iterações.\\n\\n\\n'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" \n",
    "    ========================== OBJETIVOS DA IA ==============================\n",
    "\n",
    "    ACURACY : VALOR PRÓXIMO A 1\n",
    "        Interpretação: A acurácia mede a proporção de previsões corretas do modelo em relação ao total de previsões feitas. O valor ideal seria 1 (ou 100%), o que significa que o modelo acertou todas as previsões.\n",
    "        Em problemas de classificação, uma acurácia de 1 (ou 100%) significa que o modelo está fazendo previsões perfeitas.\n",
    "    LOSS (PERDA) : VALOR PRÓXIMO A 0\n",
    "        Interpretação: A função de perda (loss function) calcula a diferença entre as previsões do modelo e os valores reais (verdadeiros). Quanto menor for o valor da perda, melhor o modelo está se ajustando aos dados.\n",
    "        Um valor de 0 indica que o modelo está fazendo previsões perfeitas, ou seja, sem erro. Na prática, o valor de perda não é geralmente 0, mas deve ser o menor possível.\n",
    "\n",
    "    ========================== DICAS ==============================\n",
    "\n",
    "    ALFA (TAXA DE APRENDIZADO) : \n",
    "        VALOR ALTO : Comportamento: Se o valor de alfa for muito alto, o modelo pode fazer grandes atualizações nos pesos em cada iteração. Isso pode levar a uma convergência instável, onde o modelo \"salta\" para frente e para trás sem realmente encontrar o ponto ótimo.\n",
    "\n",
    "        VALOR BAIXO : Se o valor de alfa for muito baixo, o modelo fará atualizações muito pequenas nos pesos, o que pode levar a um processo de treinamento muito lento. Isso pode aumentar o tempo de treinamento e, em alguns casos, pode fazer com que o modelo demore mais para alcançar o ponto ótimo.\n",
    "\n",
    "        IDEAL: Ajuste dinâmico: Muitas vezes, usa-se um learning rate scheduler ou técnicas como learning rate annealing ou Adam, que ajustam automaticamente a taxa de aprendizado durante o treinamento, começando com uma taxa de aprendizado relativamente alta e diminuindo ao longo das iterações.\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\n\\n    models: Construção e gerenciamento de modelos.\\n    layers: Criação de camadas para a rede neural (conv, dense, etc.).\\n    activations: Funções de ativação usadas nas camadas (ReLU, Softmax, etc.).\\n    optimizers: Otimizadores usados para treinar o modelo (Adam, SGD, etc.).\\n    utils: Funções auxiliares, como o carregamento de dados.\\n    losses: Funções de perda para calcular o erro do modelo (crossentropy, MSE, etc.).\\n    initializers: Inicialização dos pesos das camadas.\\n    metrics: Métricas para avaliação do desempenho do modelo (acurácia, precisão, recall, etc.).\\n    callbacks: Funções para controlar o treinamento (early stopping, checkpoint, etc.).\\n\\n'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importa várias partes da biblioteca Keras, que é um módulo de alto nível da biblioteca TensorFlow para a construção e treinamento de redes neurais. \n",
    "from tensorflow.keras import models, layers, activations, optimizers, utils, losses, initializers, metrics, callbacks # type: ignore\n",
    "\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 32 # tamanho da separação do sgd. Cada vez que roda ele atualiza os pessos da rede, entt ele vai atualizar os pesos 32 vezes, mtt pequeno é mais rápido porém mais instável.\n",
    "patience = 10 # quanto o earlystop vai esperar para parar, nesse caso são 5 epochs\n",
    "learning_rate = 0.0001 # alfa do SGD\n",
    "model_path = 'model/model.keras' # onde ele vai salvar o modelo, o 'dump'\n",
    "\n",
    "\"\"\" \n",
    "\n",
    "    models: Construção e gerenciamento de modelos.\n",
    "    layers: Criação de camadas para a rede neural (conv, dense, etc.).\n",
    "    activations: Funções de ativação usadas nas camadas (ReLU, Softmax, etc.).\n",
    "    optimizers: Otimizadores usados para treinar o modelo (Adam, SGD, etc.).\n",
    "    utils: Funções auxiliares, como o carregamento de dados.\n",
    "    losses: Funções de perda para calcular o erro do modelo (crossentropy, MSE, etc.).\n",
    "    initializers: Inicialização dos pesos das camadas.\n",
    "    metrics: Métricas para avaliação do desempenho do modelo (acurácia, precisão, recall, etc.).\n",
    "    callbacks: Funções para controlar o treinamento (early stopping, checkpoint, etc.).\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  \\n    ====================== Explicações ==========================\\n    camada Dense:\\n    Função: A camada Dense é uma camada totalmente conectada, ou seja, cada neurônio da camada anterior está conectado a todos os neurônios da camada atual.\\n        Aplicação: Essa camada é usada principalmente em redes neurais feedforward (como redes densas ou MLP - Multi-Layer Perceptron), onde os dados são passados por todas as camadas de forma sequencial.\\n        Entrada/saída: A entrada para uma camada Dense pode ser de qualquer dimensão, mas é geralmente um vetor unidimensional (por exemplo, uma série de características de um dado ou um vetor achatado de uma imagem).\\n    Camada Conv2d: \\n        função: A camada Conv2D realiza uma operação matemática chamada convolução em duas dimensões (como imagens ou qualquer dado que tenha altura e largura, como uma matriz 2D). Ela é usada para detectar padrões, bordas, texturas ou outras características em uma imagem, ou qualquer tipo de entrada que tenha duas dimensões.\\n\\n        Uso: camadas Conv2D e MaxPooling2D são usadas para operar em dados de duas dimensões (como imagens). Se você estiver lidando com imagens, onde as entradas são matrizes 2D (como imagens RGB).\\n\\n        # Como funciona:\\n\\n            Filtros (ou Kernels): A camada Conv2D usa um conjunto de filtros (ou kernels) que deslizam sobre a imagem de entrada. Cada filtro é uma pequena matriz que tem a função de \"examinar\" uma região da imagem para extrair informações, como bordas, texturas, etc.\\n\\n            Deslizamento (Stride): Quando o filtro desliza pela imagem (ou qualquer dado 2D), ele se move de acordo com um valor chamado \"stride\". Isso determina o quão longe o filtro se move a cada vez (por exemplo, se o stride for 1, o filtro move uma célula por vez).\\n\\n            Padding: A operação de convolução pode ser feita com ou sem \"padding\" (preenchimento). O padding é quando a imagem de entrada é \"esticada\" com pixels extras (geralmente zero) ao redor, para garantir que o filtro possa ser aplicado nas bordas da imagem sem perder informações.\\n\\n            Saída (Feature Map): O resultado da convolução é uma nova matriz, chamada de feature map, que contém as informações extraídas pela convolução, como padrões e características encontradas na imagem.\\n        # Para que serve:\\n\\n            Extração de características: A principal função da convolução é extrair características (como bordas, formas, cores, texturas, etc.) de imagens ou outros dados 2D. Em uma rede neural, essas características extraídas podem ser usadas para tarefas como classificação, detecção e segmentação.\\n            \\n            Redução de dimensões: Como a convolução reduz a resolução da imagem (dependendo do stride e do padding), isso ajuda a reduzir a complexidade computacional e extrair apenas as informações mais importantes da imagem.\\n\\n'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = 10  # Define corretamente o número de classes\n",
    "\n",
    "model = models.Sequential([ # Modelo sequencial, onde as camadas são empilhadas\n",
    "    # diminuir para fazer covuluções sem dar milhões de parâmetros\n",
    "    layers.Resizing(56, 56), # colocando todas as imagens do mesmo tamanho, nn corta a imagem apenas diminui a resolução    \n",
    "    layers.Rescaling(1.0/255), # deixar todos os valores dos pixels entre 0 e 1\n",
    "    layers.RandomRotation((-0.2 , 0.2)), # leve rotação para cada imagem, rotacionar deixa mais lento mas aumenta a precisão\n",
    "\n",
    "\n",
    "    # ============== COVULAÇÃO ==============\n",
    "\n",
    "    # 32 filtros 3 por 3, tenho 32 imagens 54 por 54\n",
    "    layers.Conv2D(64, (3, 3), activation = 'relu', kernel_initializer = initializers.RandomNormal()), # valor aleatório com distribuição normal \n",
    "    \n",
    "    layers.MaxPooling2D((4, 4)), # 32 imagens 27 por 27, pooling pega cada 4 quadradinhos e pega o valor máximo de cada quadradinho, isso diminui a lentidão\n",
    "\n",
    "    layers.Conv2D(64, (4, 4), activation = 'relu', kernel_initializer = initializers.RandomNormal()),\n",
    "                  \n",
    "    layers.MaxPooling2D((4, 4)),\n",
    "                  \n",
    "    layers.Flatten(), # vai pegar tds as imagens 9 x 8 e transformar num vetor gigante\n",
    "\n",
    "    # layers.Dropout(0.5), # desativa 20% dos dados na camad\n",
    "    \n",
    "    # ==================== CAMADAS =====================\n",
    "                  \n",
    "    # primeira camada\n",
    "    # Adiciona uma camada densa com 128 neurônios\n",
    "    layers.Dense(256, activation = layers.LeakyReLU(alpha=0.01), kernel_initializer = initializers.RandomNormal()),\n",
    "\n",
    "    layers.Dropout(0.2), # desativa 20% dos dados na camada\n",
    "\n",
    "    #  2 camada (camda oculta) \n",
    "    layers.Dense(256, activation = layers.LeakyReLU(alpha=0.00), kernel_initializer = initializers.RandomNormal()),\n",
    "\n",
    "    layers.Dropout(0.2), # desativa 20% dos dados na camada\n",
    "    layers.Dense(128, activation = layers.LeakyReLU(alpha=0.01), kernel_initializer = initializers.RandomNormal()),\n",
    "\n",
    "    layers.Dense(128, activation = layers.LeakyReLU(alpha=0.01), kernel_initializer = initializers.RandomNormal()),\n",
    "\n",
    "    layers.Dense(128, activation = layers.LeakyReLU(alpha=0.01), kernel_initializer = initializers.RandomNormal()),\n",
    "\n",
    "    layers.Dropout(0.2), # desativa 20% dos dados na camada\n",
    "    layers.Dense(64, activation = layers.LeakyReLU(alpha=0.001), kernel_initializer = initializers.RandomNormal()),\n",
    "\n",
    "    # camada de saída\n",
    "    layers.Dense(num_classes, activation='softmax')  # Ajustado para múltiplas classes\n",
    "])\n",
    "\n",
    "\"\"\"  \n",
    "    ====================== Explicações ==========================\n",
    "    camada Dense:\n",
    "    Função: A camada Dense é uma camada totalmente conectada, ou seja, cada neurônio da camada anterior está conectado a todos os neurônios da camada atual.\n",
    "        Aplicação: Essa camada é usada principalmente em redes neurais feedforward (como redes densas ou MLP - Multi-Layer Perceptron), onde os dados são passados por todas as camadas de forma sequencial.\n",
    "        Entrada/saída: A entrada para uma camada Dense pode ser de qualquer dimensão, mas é geralmente um vetor unidimensional (por exemplo, uma série de características de um dado ou um vetor achatado de uma imagem).\n",
    "    Camada Conv2d: \n",
    "        função: A camada Conv2D realiza uma operação matemática chamada convolução em duas dimensões (como imagens ou qualquer dado que tenha altura e largura, como uma matriz 2D). Ela é usada para detectar padrões, bordas, texturas ou outras características em uma imagem, ou qualquer tipo de entrada que tenha duas dimensões.\n",
    "\n",
    "        Uso: camadas Conv2D e MaxPooling2D são usadas para operar em dados de duas dimensões (como imagens). Se você estiver lidando com imagens, onde as entradas são matrizes 2D (como imagens RGB).\n",
    "\n",
    "        # Como funciona:\n",
    "\n",
    "            Filtros (ou Kernels): A camada Conv2D usa um conjunto de filtros (ou kernels) que deslizam sobre a imagem de entrada. Cada filtro é uma pequena matriz que tem a função de \"examinar\" uma região da imagem para extrair informações, como bordas, texturas, etc.\n",
    "\n",
    "            Deslizamento (Stride): Quando o filtro desliza pela imagem (ou qualquer dado 2D), ele se move de acordo com um valor chamado \"stride\". Isso determina o quão longe o filtro se move a cada vez (por exemplo, se o stride for 1, o filtro move uma célula por vez).\n",
    "\n",
    "            Padding: A operação de convolução pode ser feita com ou sem \"padding\" (preenchimento). O padding é quando a imagem de entrada é \"esticada\" com pixels extras (geralmente zero) ao redor, para garantir que o filtro possa ser aplicado nas bordas da imagem sem perder informações.\n",
    "\n",
    "            Saída (Feature Map): O resultado da convolução é uma nova matriz, chamada de feature map, que contém as informações extraídas pela convolução, como padrões e características encontradas na imagem.\n",
    "        # Para que serve:\n",
    "\n",
    "            Extração de características: A principal função da convolução é extrair características (como bordas, formas, cores, texturas, etc.) de imagens ou outros dados 2D. Em uma rede neural, essas características extraídas podem ser usadas para tarefas como classificação, detecção e segmentação.\n",
    "            \n",
    "            Redução de dimensões: Como a convolução reduz a resolução da imagem (dependendo do stride e do padding), isso ajuda a reduzir a complexidade computacional e extrair apenas as informações mais importantes da imagem.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer = optimizers.Adam( # adam é um algoritmo keras\n",
    "        learning_rate = learning_rate\n",
    "    ),\n",
    "    # loss=losses.CategoricalCrossentropy(), # mais de uma classe, quando nn é um int a classe\n",
    "    loss=losses.SparseCategoricalCrossentropy(), # quando é int a classe\n",
    "    # metrics = [metrics.BinaryAccuracy(), metrics.Precision(), metrics.Recall() ] # para uma classe só, 1 ou 0\n",
    "    metrics=['accuracy']  # Usa accuracy normal para multiclasse\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 733 files belonging to 10 classes.\n",
      "Using 587 files for training.\n",
      "Found 733 files belonging to 10 classes.\n",
      "Using 146 files for validation.\n",
      "['bougainvillea', 'daisies', 'garden', 'gardenias', 'hibiscus', 'hydrangeas', 'lilies', 'orchids', 'peonies', 'tulip']\n"
     ]
    }
   ],
   "source": [
    "train = utils.image_dataset_from_directory(\n",
    "    output_path,\n",
    "    validation_split = 0.2,\n",
    "    subset= 'training',\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    image_size=(244,244),\n",
    "    batch_size = batch_size\n",
    ")\n",
    "\n",
    "test = utils.image_dataset_from_directory(\n",
    "    output_path,\n",
    "    validation_split = 0.2,\n",
    "    subset= 'validation',\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    image_size=(244,244),\n",
    "    batch_size = batch_size\n",
    ")\n",
    "\n",
    "print(train.class_names)\n",
    "\n",
    "# Se as imagens forem RGB, o formato precisa ser (batch_size, altura, largura, 3). Para conferir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 87ms/step - accuracy: 0.0893 - loss: 2.3027 - val_accuracy: 0.1233 - val_loss: 2.3020\n",
      "Epoch 2/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 78ms/step - accuracy: 0.1344 - loss: 2.3021 - val_accuracy: 0.1096 - val_loss: 2.3017\n",
      "Epoch 3/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.1140 - loss: 2.3016 - val_accuracy: 0.1096 - val_loss: 2.3012\n",
      "Epoch 4/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.1407 - loss: 2.2999 - val_accuracy: 0.1096 - val_loss: 2.3000\n",
      "Epoch 5/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.1363 - loss: 2.2976 - val_accuracy: 0.1096 - val_loss: 2.2972\n",
      "Epoch 6/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.1473 - loss: 2.2924 - val_accuracy: 0.1164 - val_loss: 2.2906\n",
      "Epoch 7/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.1424 - loss: 2.2810 - val_accuracy: 0.1438 - val_loss: 2.2639\n",
      "Epoch 8/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.1899 - loss: 2.2429 - val_accuracy: 0.1849 - val_loss: 2.1752\n",
      "Epoch 9/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - accuracy: 0.2254 - loss: 2.1364 - val_accuracy: 0.1986 - val_loss: 2.0733\n",
      "Epoch 10/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 80ms/step - accuracy: 0.2088 - loss: 2.0302 - val_accuracy: 0.1781 - val_loss: 2.0185\n",
      "Epoch 11/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.1837 - loss: 1.9731 - val_accuracy: 0.1918 - val_loss: 1.9934\n",
      "Epoch 12/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 93ms/step - accuracy: 0.2352 - loss: 1.9359 - val_accuracy: 0.2123 - val_loss: 1.9817\n",
      "Epoch 13/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 96ms/step - accuracy: 0.2404 - loss: 1.9020 - val_accuracy: 0.2192 - val_loss: 1.9314\n",
      "Epoch 14/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.2226 - loss: 1.8835 - val_accuracy: 0.2740 - val_loss: 1.9230\n",
      "Epoch 15/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 83ms/step - accuracy: 0.2564 - loss: 1.8464 - val_accuracy: 0.2603 - val_loss: 1.9069\n",
      "Epoch 16/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 81ms/step - accuracy: 0.2476 - loss: 1.8452 - val_accuracy: 0.2740 - val_loss: 1.8963\n",
      "Epoch 17/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.2346 - loss: 1.8225 - val_accuracy: 0.2397 - val_loss: 1.8899\n",
      "Epoch 18/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.2737 - loss: 1.8120 - val_accuracy: 0.2740 - val_loss: 1.8816\n",
      "Epoch 19/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.2423 - loss: 1.8491 - val_accuracy: 0.2808 - val_loss: 1.8876\n",
      "Epoch 20/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.2741 - loss: 1.8373 - val_accuracy: 0.2671 - val_loss: 1.9235\n",
      "Epoch 21/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 82ms/step - accuracy: 0.2573 - loss: 1.8358 - val_accuracy: 0.2603 - val_loss: 1.8849\n",
      "Epoch 22/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 93ms/step - accuracy: 0.2990 - loss: 1.7931 - val_accuracy: 0.2808 - val_loss: 1.8742\n",
      "Epoch 23/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - accuracy: 0.2944 - loss: 1.8280 - val_accuracy: 0.2808 - val_loss: 1.8873\n",
      "Epoch 24/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 83ms/step - accuracy: 0.3082 - loss: 1.7880 - val_accuracy: 0.2740 - val_loss: 1.8750\n",
      "Epoch 25/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.2788 - loss: 1.8034 - val_accuracy: 0.2945 - val_loss: 1.8723\n",
      "Epoch 26/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.2357 - loss: 1.8034 - val_accuracy: 0.2740 - val_loss: 1.8663\n",
      "Epoch 27/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.2862 - loss: 1.7881 - val_accuracy: 0.2808 - val_loss: 1.8599\n",
      "Epoch 28/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 97ms/step - accuracy: 0.3131 - loss: 1.8007 - val_accuracy: 0.2603 - val_loss: 1.8681\n",
      "Epoch 29/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.2887 - loss: 1.7782 - val_accuracy: 0.2740 - val_loss: 1.8719\n",
      "Epoch 30/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 104ms/step - accuracy: 0.3112 - loss: 1.7730 - val_accuracy: 0.2055 - val_loss: 1.9220\n",
      "Epoch 31/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.2964 - loss: 1.7544 - val_accuracy: 0.3082 - val_loss: 1.8606\n",
      "Epoch 32/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.2844 - loss: 1.7870 - val_accuracy: 0.2055 - val_loss: 1.9555\n",
      "Epoch 33/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - accuracy: 0.2668 - loss: 1.8941 - val_accuracy: 0.2329 - val_loss: 1.8843\n",
      "Epoch 34/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 93ms/step - accuracy: 0.2659 - loss: 1.7905 - val_accuracy: 0.2740 - val_loss: 1.8610\n",
      "Epoch 35/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 100ms/step - accuracy: 0.3071 - loss: 1.7736 - val_accuracy: 0.2740 - val_loss: 1.8736\n",
      "Epoch 36/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.2944 - loss: 1.7524 - val_accuracy: 0.2534 - val_loss: 1.8689\n",
      "Epoch 37/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 92ms/step - accuracy: 0.2961 - loss: 1.7830 - val_accuracy: 0.2740 - val_loss: 1.8569\n",
      "Epoch 38/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - accuracy: 0.3102 - loss: 1.7424 - val_accuracy: 0.2808 - val_loss: 1.8538\n",
      "Epoch 39/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 101ms/step - accuracy: 0.3003 - loss: 1.7525 - val_accuracy: 0.2808 - val_loss: 1.8423\n",
      "Epoch 40/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 92ms/step - accuracy: 0.3079 - loss: 1.7482 - val_accuracy: 0.2808 - val_loss: 1.8419\n",
      "Epoch 41/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.3292 - loss: 1.7190 - val_accuracy: 0.2740 - val_loss: 1.8593\n",
      "Epoch 42/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 100ms/step - accuracy: 0.3163 - loss: 1.7294 - val_accuracy: 0.2671 - val_loss: 1.8457\n",
      "Epoch 43/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 98ms/step - accuracy: 0.3178 - loss: 1.7657 - val_accuracy: 0.2808 - val_loss: 1.8544\n",
      "Epoch 44/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 96ms/step - accuracy: 0.3426 - loss: 1.7770 - val_accuracy: 0.2877 - val_loss: 1.8402\n",
      "Epoch 45/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.2917 - loss: 1.7831 - val_accuracy: 0.2877 - val_loss: 1.8373\n",
      "Epoch 46/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 102ms/step - accuracy: 0.3399 - loss: 1.7315 - val_accuracy: 0.2740 - val_loss: 1.8336\n",
      "Epoch 47/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.3092 - loss: 1.7621 - val_accuracy: 0.2603 - val_loss: 1.8830\n",
      "Epoch 48/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 93ms/step - accuracy: 0.3078 - loss: 1.7424 - val_accuracy: 0.2397 - val_loss: 1.8383\n",
      "Epoch 49/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - accuracy: 0.3406 - loss: 1.7247 - val_accuracy: 0.2877 - val_loss: 1.8279\n",
      "Epoch 50/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.2877 - loss: 1.7808 - val_accuracy: 0.3151 - val_loss: 1.8414\n",
      "Epoch 51/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 92ms/step - accuracy: 0.3563 - loss: 1.7429 - val_accuracy: 0.2808 - val_loss: 1.8443\n",
      "Epoch 52/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 91ms/step - accuracy: 0.2772 - loss: 1.7904 - val_accuracy: 0.2808 - val_loss: 1.8251\n",
      "Epoch 53/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.3019 - loss: 1.7191 - val_accuracy: 0.2671 - val_loss: 1.8545\n",
      "Epoch 54/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 91ms/step - accuracy: 0.3130 - loss: 1.7531 - val_accuracy: 0.2877 - val_loss: 1.8448\n",
      "Epoch 55/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.3504 - loss: 1.7203 - val_accuracy: 0.3014 - val_loss: 1.8217\n",
      "Epoch 56/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.3279 - loss: 1.7271 - val_accuracy: 0.3014 - val_loss: 1.8263\n",
      "Epoch 57/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.3463 - loss: 1.7474 - val_accuracy: 0.2740 - val_loss: 1.8256\n",
      "Epoch 58/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.3460 - loss: 1.6804 - val_accuracy: 0.2945 - val_loss: 1.8156\n",
      "Epoch 59/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.2939 - loss: 1.7643 - val_accuracy: 0.2877 - val_loss: 1.8168\n",
      "Epoch 60/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 91ms/step - accuracy: 0.3288 - loss: 1.7108 - val_accuracy: 0.2808 - val_loss: 1.8377\n",
      "Epoch 61/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 93ms/step - accuracy: 0.3194 - loss: 1.7268 - val_accuracy: 0.2397 - val_loss: 1.8433\n",
      "Epoch 62/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.3213 - loss: 1.7024 - val_accuracy: 0.2603 - val_loss: 1.8129\n",
      "Epoch 63/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 92ms/step - accuracy: 0.3482 - loss: 1.6865 - val_accuracy: 0.2945 - val_loss: 1.7972\n",
      "Epoch 64/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 97ms/step - accuracy: 0.3679 - loss: 1.6876 - val_accuracy: 0.2808 - val_loss: 1.8019\n",
      "Epoch 65/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.3367 - loss: 1.7018 - val_accuracy: 0.3082 - val_loss: 1.7982\n",
      "Epoch 66/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - accuracy: 0.3552 - loss: 1.7174 - val_accuracy: 0.3082 - val_loss: 1.7788\n",
      "Epoch 67/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - accuracy: 0.3270 - loss: 1.6946 - val_accuracy: 0.2945 - val_loss: 1.8204\n",
      "Epoch 68/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 100ms/step - accuracy: 0.3596 - loss: 1.6565 - val_accuracy: 0.3151 - val_loss: 1.7914\n",
      "Epoch 69/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 101ms/step - accuracy: 0.3499 - loss: 1.6661 - val_accuracy: 0.3151 - val_loss: 1.7534\n",
      "Epoch 70/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 99ms/step - accuracy: 0.3897 - loss: 1.6273 - val_accuracy: 0.3219 - val_loss: 1.7619\n",
      "Epoch 71/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.3345 - loss: 1.6680 - val_accuracy: 0.3288 - val_loss: 1.7531\n",
      "Epoch 72/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.3630 - loss: 1.6358 - val_accuracy: 0.3082 - val_loss: 1.7589\n",
      "Epoch 73/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - accuracy: 0.3624 - loss: 1.6689 - val_accuracy: 0.3356 - val_loss: 1.7489\n",
      "Epoch 74/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.3685 - loss: 1.6217 - val_accuracy: 0.3425 - val_loss: 1.7312\n",
      "Epoch 75/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 91ms/step - accuracy: 0.4008 - loss: 1.6048 - val_accuracy: 0.3288 - val_loss: 1.7278\n",
      "Epoch 76/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.3915 - loss: 1.6029 - val_accuracy: 0.3151 - val_loss: 1.7266\n",
      "Epoch 77/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - accuracy: 0.4195 - loss: 1.5489 - val_accuracy: 0.3288 - val_loss: 1.7302\n",
      "Epoch 78/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.3860 - loss: 1.6196 - val_accuracy: 0.3425 - val_loss: 1.6865\n",
      "Epoch 79/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 96ms/step - accuracy: 0.3971 - loss: 1.5364 - val_accuracy: 0.3288 - val_loss: 1.6819\n",
      "Epoch 80/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 103ms/step - accuracy: 0.4329 - loss: 1.5516 - val_accuracy: 0.3356 - val_loss: 1.6745\n",
      "Epoch 81/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 96ms/step - accuracy: 0.4320 - loss: 1.5531 - val_accuracy: 0.3425 - val_loss: 1.6984\n",
      "Epoch 82/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 99ms/step - accuracy: 0.4215 - loss: 1.5647 - val_accuracy: 0.3014 - val_loss: 1.6957\n",
      "Epoch 83/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - accuracy: 0.4132 - loss: 1.5558 - val_accuracy: 0.3630 - val_loss: 1.6629\n",
      "Epoch 84/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step - accuracy: 0.4445 - loss: 1.5247 - val_accuracy: 0.3836 - val_loss: 1.6628\n",
      "Epoch 85/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 99ms/step - accuracy: 0.4532 - loss: 1.4922 - val_accuracy: 0.3699 - val_loss: 1.6448\n",
      "Epoch 86/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - accuracy: 0.4510 - loss: 1.4670 - val_accuracy: 0.4041 - val_loss: 1.6561\n",
      "Epoch 87/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.4556 - loss: 1.5429 - val_accuracy: 0.4247 - val_loss: 1.6152\n",
      "Epoch 88/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - accuracy: 0.4530 - loss: 1.5208 - val_accuracy: 0.3973 - val_loss: 1.6103\n",
      "Epoch 89/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 99ms/step - accuracy: 0.4452 - loss: 1.5132 - val_accuracy: 0.4110 - val_loss: 1.6484\n",
      "Epoch 90/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 93ms/step - accuracy: 0.4546 - loss: 1.4333 - val_accuracy: 0.4110 - val_loss: 1.6345\n",
      "Epoch 91/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step - accuracy: 0.4500 - loss: 1.4932 - val_accuracy: 0.3699 - val_loss: 1.6561\n",
      "Epoch 92/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.4614 - loss: 1.5232 - val_accuracy: 0.4041 - val_loss: 1.6377\n",
      "Epoch 93/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 83ms/step - accuracy: 0.4552 - loss: 1.4994 - val_accuracy: 0.3562 - val_loss: 1.6344\n",
      "Epoch 94/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.4965 - loss: 1.4471 - val_accuracy: 0.3904 - val_loss: 1.6492\n",
      "Epoch 95/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.4460 - loss: 1.5186 - val_accuracy: 0.4110 - val_loss: 1.6131\n",
      "Epoch 96/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - accuracy: 0.4701 - loss: 1.4852 - val_accuracy: 0.4315 - val_loss: 1.5889\n",
      "Epoch 97/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 83ms/step - accuracy: 0.4856 - loss: 1.3783 - val_accuracy: 0.4384 - val_loss: 1.5880\n",
      "Epoch 98/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.4928 - loss: 1.3851 - val_accuracy: 0.4452 - val_loss: 1.6004\n",
      "Epoch 99/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 93ms/step - accuracy: 0.4798 - loss: 1.4059 - val_accuracy: 0.4384 - val_loss: 1.5897\n",
      "Epoch 100/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 92ms/step - accuracy: 0.4780 - loss: 1.4179 - val_accuracy: 0.4247 - val_loss: 1.5946\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x23ff601c470>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "    \n",
    "model.fit(\n",
    "    train,\n",
    "    validation_data = test,\n",
    "    epochs = epochs,\n",
    "    callbacks = [\n",
    "        callbacks.EarlyStopping(\n",
    "            monitor = 'val_loss',\n",
    "            patience = patience\n",
    "        ),\n",
    "        callbacks.ModelCheckpoint(\n",
    "            filepath = model_path,\n",
    "            save_weights_only = False,\n",
    "            monitor = 'loss',\n",
    "            mode = 'min',\n",
    "            save_best_only = True\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
